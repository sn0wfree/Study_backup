mainMonteCarlo<-function(i){
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
##pre-cal:heteroscedasticity
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
parLapply(cl,1:loop*10,mainMonteCarlo)
}
require(lmtest)
require(MASS)
require(stats)
require(nlme)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#preparing for Parallelization
require(parallel)
beta_1<<-0.4
beta_0<<-1
original_N<<-10
signlevel<<-0.05
#initial container for Wald, LM, LR
W_count<<-rep(0,loop)
LM_count<<-rep(0,loop)
LR_count<<-rep(0,loop)
P.value_homo_count<<-rep(0,loop)
# for loop start:Monte Carlo
for(j in 1:loop){#first for-loop for generating multi-sample
W<<-0
LM<<-0
LR<<-0
N<<-original_N+j
#generation part:data
x1<<-rchisq(N, 2)
x2<<-runif(N,0,10)
#for (i in 1:loop*10){# second for-loop: the main Monte Carlo coding
#initializing the Parallelization
cl <- makeCluster(detectCores(logical=FALSE))
mainMonteCarlo<-function(i){
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
##pre-cal:heteroscedasticity
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
parLapply(cl,1:loop*10,mainMonteCarlo)
}
require(lmtest)
require(MASS)
require(stats)
require(nlme)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#preparing for Parallelization
require(parallel)
beta_1<<-0.4
beta_0<<-1
original_N<<-10
signlevel<<-0.05
#initial container for Wald, LM, LR
W_count<<-rep(0,loop)
LM_count<<-rep(0,loop)
LR_count<<-rep(0,loop)
P.value_homo_count<<-rep(0,loop)
# for loop start:Monte Carlo
for(j in 1:loop){#first for-loop for generating multi-sample
W<<-0
LM<<-0
LR<<-0
N<<-original_N+j
#generation part:data
x1<<-rchisq(N, 2)
x2<<-runif(N,0,10)
#for (i in 1:loop*10){# second for-loop: the main Monte Carlo coding
#initializing the Parallelization
cl <- makeCluster(detectCores(logical=FALSE))
mainMonteCarlo<-function(i){
y=1+0.4*x1+(1-0.4)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
##pre-cal:heteroscedasticity
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
parLapply(cl,1:loop*10,mainMonteCarlo)
}
W<-0
LM<-0
LR<-0
N<-original_N+j
#generation part:data
x1<-rchisq(N, 2)
x2<-runif(N,0,10)
beta_1<-0.4
beta_0<-1
original_N<-10
signlevel<-0.05
#initial container for Wald, LM, LR
W_count<-rep(0,loop)
LM_count<-rep(0,loop)
LR_count<-rep(0,loop)
P.value_homo_count<-rep(0,loop)
input_cell<-c(original_N,N,signlevel,beta_0,beta_1,W,LM,LR,x1,x2)
input_cell
input_cell<-cbind(original_N,N,signlevel,beta_0,beta_1,W,LM,LR,x1,x2)
input_cell
N<-input_cell[1][1]
N
aw<-function(xx){return(xx*xx)}
w<-apply(1:1000,aw)
w<-Lapply(1:1000,aw)
w<-lapply(1:1000,aw)
a
w
w
length(w)
require(lmtest)
require(MASS)
require(stats)
require(nlme)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
beta_1=0.4
beta_0=1
#x1_store=rchisq(80+20, 2)
#initial valueset
original_N=10
signlevel=0.05
#initial container for Wald, LM, LR
W_count=rep(0,loop)
LM_count=rep(0,loop)
LR_count=rep(0,loop)
P.value_homo_count=rep(0,loop)
# for loop start:Monte Carlo
for(j in 1:loop){#first for-loop for generating multi-sample
W=0
LM=0
LR=0
N=original_N+j
#generation part:data
x1=rchisq(N, 2)
x2=runif(N,0,10)
for (i in 1:loop*50){# second for-loop: the main Monte Carlo coding
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
#heter
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
}
stopCluster(cl)
require(lmtest)
require(MASS)
require(stats)
require(nlme)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
beta_1=0.4
beta_0=1
#x1_store=rchisq(80+20, 2)
#initial valueset
original_N=10
signlevel=0.05
#initial container for Wald, LM, LR
W_count=rep(0,loop)
LM_count=rep(0,loop)
LR_count=rep(0,loop)
P.value_homo_count=rep(0,loop)
# for loop start:Monte Carlo
for(j in 1:loop){#first for-loop for generating multi-sample
W=0
LM=0
LR=0
N=original_N+j
#generation part:data
x1=rchisq(N, 2)
x2=runif(N,0,10)
for (i in 1:loop*50){# second for-loop: the main Monte Carlo coding
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
#heter
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
}
plot(P.value_homo_count/loop)
plot(W_count/loop)
plot(LM_count/loop)
plot(LR_count/loop)
require(lmtest)
require(MASS)
require(stats)
require(nlme)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
beta_1=0.4
beta_0=1
#x1_store=rchisq(80+20, 2)
#initial valueset
original_N=10
signlevel=0.05
#initial container for Wald, LM, LR
W_count=rep(0,loop)
LM_count=rep(0,loop)
LR_count=rep(0,loop)
P.value_homo_count=rep(0,loop)
# for loop start:Monte Carlo
for(j in 1:loop){#first for-loop for generating multi-sample
W=0
LM=0
LR=0
N=original_N+j
#generation part:data
x1=rchisq(N, 2)
x2=runif(N,0,10)
for (i in 1:loop*100){# second for-loop: the main Monte Carlo coding
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
#heter
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
}
plot(P.value_homo_count/loop)
plot(W_count/loop)
plot(LM_count/loop)
plot(LR_count/loop)
require(lmtest)
require(MASS)
require(stats)
require(nlme)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=100
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
beta_1=0.4
beta_0=1
#x1_store=rchisq(80+20, 2)
#initial valueset
original_N=10
signlevel=0.05
#initial container for Wald, LM, LR
W_count=rep(0,loop)
LM_count=rep(0,loop)
LR_count=rep(0,loop)
P.value_homo_count=rep(0,loop)
# for loop start:Monte Carlo
for(j in 1:loop){#first for-loop for generating multi-sample
W=0
LM=0
LR=0
N=original_N+j
#generation part:data
x1=rchisq(N, 2)
x2=runif(N,0,10)
for (i in 1:loop*100){# second for-loop: the main Monte Carlo coding
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
#heter
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
}
require(lmtest)
require(MASS)
require(stats)
require(nlme)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=100
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
beta_1=0.4
beta_0=1
#x1_store=rchisq(80+20, 2)
#initial valueset
original_N=10
signlevel=0.05
#initial container for Wald, LM, LR
W_count=rep(0,loop)
LM_count=rep(0,loop)
LR_count=rep(0,loop)
P.value_homo_count=rep(0,loop)
# for loop start:Monte Carlo
for(j in 1:loop){#first for-loop for generating multi-sample
W=0
LM=0
LR=0
N=original_N+j
#generation part:data
x1=rchisq(N, 2)
x2=runif(N,0,10)
for (i in 1:loop*10){# second for-loop: the main Monte Carlo coding
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
#heter
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
}
require(lmtest)
require(MASS)
require(stats)
require(nlme)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=100
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
beta_1=0.4
beta_0=1
#x1_store=rchisq(80+20, 2)
#initial valueset
original_N=10
signlevel=0.05
#initial container for Wald, LM, LR
W_count=rep(0,loop)
LM_count=rep(0,loop)
LR_count=rep(0,loop)
P.value_homo_count=rep(0,loop)
# for loop start:Monte Carlo
for(j in 1:loop){#first for-loop for generating multi-sample
W=0
LM=0
LR=0
N=original_N+j
#generation part:data
x1=rchisq(N, 2)
x2=runif(N,0,10)
for (i in 1:loop*10){# second for-loop: the main Monte Carlo coding
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*rt(N,6)
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
#heter
if (bptest(resid(equ1)^2~x1*x2+x1^2+x2^2)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=1/x1^.5)
equ2<-lm((y-x2)~(x1-x2),weights=1/x1^.5)
}
#calculation
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W[i]=N*((SSRr-SSRu)/(SSRu))
LM[i]=N*((SSRr-SSRu)/(SSRr))
LR[i]=N*(log(SSRr/SSRu))
#if (bptest(equ1,studentize = 0)$p.value<signlevel){P.value_homo_count[j]=P.value_homo_count[j]+1}
P.value_homo_count[j]=P.value_homo_count[j]+bptest(equ1,studentize = 0)$p.value
if(ks.test(W,'pchisq',1)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',1)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',1)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
}
