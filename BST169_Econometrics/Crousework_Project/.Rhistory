##boost up-end for continues
#assumption part
loop=100
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
beta_1=0.4
beta_0=1
x1_store=rchisq(80+20, 2)
#initial valueset
signlevel=0.05
W_count=rep(0,loop)
LM_count=rep(0,loop)
LR_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
W=rep(0,loop)
LM=rep(0,loop)
LR=rep(0,loop)
N=10+j
for (i in 1:loop){
#generation part:data
x1=rchisq(N, 2)
x2=runif(N,0,10)
epsilon=rt(N,6)
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(equ1)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=(1/x1^0.5))
equ2<-lm((y-x2)~(x1-x2),weights=(1/x1^0.5))
}
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W=N*((SSRr-SSRu)/(SSRu))
LM=N*((SSRr-SSRu)/(SSRr))
LR=N*(log(SSRr/SSRu))
if(ks.test(W,'pchisq',2)$p.value>signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',2)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',2)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
}
plot(W_count)
plot(LM_count)
plot(LR_count)
plot(W_count/(51:(50+loop)))
pbp=read.csv("/Users/sn0wfree/Dropbox/PhD_1st_study/BST169_Econometrics/Crousework_Project/pbp.csv")
length(pbp)
pbp
length(pbp$y)
N=40
sample=sample(1:length(pbp$y),N,replace = 1)
y<-pbp$y[sample]
y
pbp$y
sample
y
pbp$y[577]
equ1$coefficients
equ1$coefficients[2]
t.test(1)
t.test(x = 12)
t.test(x = 12,1)
t.test(x,1)
t.test(x,y)
pbp=read.csv("/Users/sn0wfree/Dropbox/PhD_1st_study/BST169_Econometrics/Crousework_Project/pbp.csv")
#head(pbp)
#str(pbp)
require(lmtest)
equ1<-lm(y~x1 + x2,data=pbp)
equ2<-lm((y-x2)~(x1-x2), data=pbp)
#Breusch-Pagan-Godfrey Test
bptest(equ1)
bptest(equ2)
#White test
bptest(residuals(equ1)~x1+x2+x1*x2+x1^2+x2^2,data=pbp)
bptest(residuals(equ2)~(x1-x2)+(x1-x2)^2,data=pbp)
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=100
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#initial valueset
original_N=10
signlevel=0.05
theta_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
N=original_N+j
for (i in 1:loop){
#generation part:data
samples=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[samples]
question5.x1<-pbp$x1[samples]
question5.x2<-pbp$x2[samples]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^0.5))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc SSR and Wald,LM, and LR
beta_1=question5.equ1$coefficients[2]
beta_2=question5.equ1$coefficients[2]
theta[i]=beta_1+beta_2
}
if(t.test(theta,rep(1,loop))$p.value>signlevel){theta_count[j]=theta_count[j]+1}
}
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=100
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#initial valueset
original_N=10
signlevel=0.05
theta_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
theta=rep(0,loop)
N=original_N+j
for (i in 1:loop){
#generation part:data
samples=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[samples]
question5.x1<-pbp$x1[samples]
question5.x2<-pbp$x2[samples]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^0.5))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc SSR and Wald,LM, and LR
beta_1=question5.equ1$coefficients[2]
beta_2=question5.equ1$coefficients[2]
theta[i]=beta_1+beta_2
}
if(t.test(theta,rep(1,loop))$p.value>signlevel){theta_count[j]=theta_count[j]+1}
}
plot(theta_count/(N+1:(N+loop)))
#generation part:data
samples=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[samples]
question5.x1<-pbp$x1[samples]
question5.x2<-pbp$x2[samples]
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
beta_1=question5.equ1$coefficients[2]
theta[i]=question5.equ1$coefficients[2]+question5.equ1$coefficients[3]
theta
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=10
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#initial valueset
original_N=10
signlevel=0.05
theta_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
theta=rep(0,loop)
N=original_N+j
for (i in 1:loop){
#generation part:data
samples=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[samples]
question5.x1<-pbp$x1[samples]
question5.x2<-pbp$x2[samples]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^0.5))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc
theta[i]=question5.equ1$coefficients[2]+question5.equ1$coefficients[3]
}
if(t.test(theta,rep(1,loop))$p.value>signlevel){theta_count[j]=theta_count[j]+1}
}
plot(theta_count/(N+1:(N+loop)))
s=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[s]
question5.x1<-pbp$x1[s]
question5.x2<-pbp$x2[s]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^0.5))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc
theta[i]=question5.equ1$coefficients[2]+question5.equ1$coefficients[3]
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=10
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#initial valueset
original_N=10
signlevel=0.05
theta_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
theta=rep(0,loop)
N=original_N+j
for (i in 1:loop){
#generation part:data
s=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[s]
question5.x1<-pbp$x1[s]
question5.x2<-pbp$x2[s]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
#epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^0.5))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc
theta[i]=question5.equ1$coefficients[2]+question5.equ1$coefficients[3]
}
if(t.test(theta,rep(1,loop))$p.value>signlevel){theta_count[j]=theta_count[j]+1}
}
plot(theta_count/(N+1:(N+loop)))
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#initial valueset
original_N=10
signlevel=0.05
theta_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
theta=rep(0,loop)
N=original_N+j
for (i in 1:loop){
#generation part:data
s=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[s]
question5.x1<-pbp$x1[s]
question5.x2<-pbp$x2[s]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
#epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^0.5))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc
theta[i]=question5.equ1$coefficients[2]+question5.equ1$coefficients[3]
}
if(t.test(theta,rep(1,loop))$p.value>signlevel){theta_count[j]=theta_count[j]+1}
}
plot(theta_count/(N+1:(N+loop)))
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#initial valueset
original_N=10
signlevel=0.05
theta_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
theta=rep(0,loop)
N=original_N+j
for (i in 1:loop){
#generation part:data
s=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[s]
question5.x1<-pbp$x1[s]
question5.x2<-pbp$x2[s]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
#epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^2))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc
theta[i]=question5.equ1$coefficients[2]+question5.equ1$coefficients[3]
}
if(t.test(theta,rep(1,loop))$p.value>signlevel){theta_count[j]=theta_count[j]+1}
}
plot(theta_count/(N+1:(N+loop)))
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#initial valueset
original_N=10
signlevel=0.05
theta_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
theta=rep(0,loop)
N=original_N+j
for (i in 1:loop){
#generation part:data
s=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[s]
question5.x1<-pbp$x1[s]
question5.x2<-pbp$x2[s]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
#epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^2))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc
theta[i]=question5.equ1$coefficients[2]+question5.equ1$coefficients[3]
}
if(t.test(theta,rep(1,loop))$p.value<signlevel){theta_count[j]=theta_count[j]+1}
}
plot(theta_count/(N+1:(N+loop)))
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=100
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
beta_1=0.4
beta_0=1
x1_store=rchisq(80+20, 2)
#initial valueset
original_N=10
signlevel=0.05
W_count=rep(0,loop)
LM_count=rep(0,loop)
LR_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
W=rep(0,loop)
LM=rep(0,loop)
LR=rep(0,loop)
N=original_N+j
for (i in 1:loop){
#generation part:data
x1=rchisq(N, 2)
x2=runif(N,0,10)
epsilon=rt(N,6)
y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
equ1<-lm(y~x1+x2)
equ2<-lm((y-x2)~(x1-x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(equ1)$p.value<signlevel){
equ1<-lm(y~x1+x2,weights=(1/x1^0.5))
equ2<-lm((y-x2)~(x1-x2),weights=(1/x1^0.5))
}
#calc SSR and Wald,LM, and LR
SSRu=sum(residuals(equ1)^2)
SSRr=sum(residuals(equ2)^2)
W=N*((SSRr-SSRu)/(SSRu))
LM=N*((SSRr-SSRu)/(SSRr))
LR=N*(log(SSRr/SSRu))
if(ks.test(W,'pchisq',2)$p.value<signlevel){W_count[j]=W_count[j]+1}
if(ks.test(LM,'pchisq',2)$p.value>signlevel){LM_count[j]=LM_count[j]+1}
if(ks.test(LR,'pchisq',2)$p.value>signlevel){LR_count[j]=LR_count[j]+1}
}
}
plot(W_count/(original_N+1:(original_N+loop)))
plot(LM_count/(original_N+1:(original_N+loop)))
plot(LR_count/(original_N+1:(original_N+loop)))
W_count
(original_N+1:(original_N+loop))
plot(W_count/(original_N+1:(original_N+loop))^2)
W_count/(original_N+1:(original_N+loop))^2
require(lmtest)
require(MASS)
require(stats)
##boost up: translate programme language code into Byte-code.
require(compiler)
enableJIT(3)
##boost up-end for continues
#assumption part
loop=50
#Warning: the loop time cannot be larger any more;please forgive me, this all my Macbook fault. And the optimization of R is terrible.
#initial valueset
original_N=10
signlevel=0.05
theta_count=rep(0,loop)
#
# for loop start:Monte Carlo
for(j in 1:loop){
theta=rep(0,loop)
N=original_N+j
for (i in 1:loop){
#generation part:data
s=sample(1:length(pbp$y),N,replace = 1)
question5.y<-pbp$y[s]
question5.x1<-pbp$x1[s]
question5.x2<-pbp$x2[s]
ee=rnorm(N,mean=0,sd=sqrt(var(question5.y)))
#epsilon=ee/sqrt(x1)
#y=beta_0+beta_1*x1+(1-beta_1)*x2+sqrt(x1)*epsilon
#generation part:regression
question5.equ1<-lm(question5.y~question5.x1+question5.x2)
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2))
#calculation
##pre-cal:heteroscedasticity
if (bptest(question5.equ1)$p.value<signlevel){
question5.equ1<-lm(question5.y~question5.x1+question5.x2,weights=(1/question5.x1^2))
#question5.equ2<-lm((question5.y-question5.x2)~(question5.x1-question5.x2),weights=(1/question5.x1^0.5))
}
#calc
theta[i]=question5.equ1$coefficients[2]+question5.equ1$coefficients[3]
}
if(t.test(theta,rep(1,loop))$p.value<signlevel){theta_count[j]=theta_count[j]+1}
}
plot(theta_count/(N+1:(N+loop)^2))
